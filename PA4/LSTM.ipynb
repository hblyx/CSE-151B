{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76282f9f-efa8-4cb3-b46b-fef1645d77f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # make notebook more visible, disable warnings\n",
    "\n",
    "from constants import config_data\n",
    "from dataset_factory import get_datasets\n",
    "from pycocotools.coco import COCO\n",
    "from matplotlib import pyplot as plt\n",
    "from model_factory import baseline_encoder, baseline_decoder\n",
    "from torchvision import transforms\n",
    "\n",
    "torch.manual_seed(128)\n",
    "np.random.seed(seed = 100) # fix the random seed to make things reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca96ca-5450-43c0-8413-9d92768242dc",
   "metadata": {},
   "source": [
    "## Setup GPU(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fcd0cd1-23d4-4f86-b8e8-7d1dd34e9f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is availiable: True\n",
      "# of GPU: 1\n",
      "Can run on GPU\n",
      "Run on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Setup GPU settings\n",
    "print(\"GPU is availiable:\", torch.cuda.is_available())\n",
    "print(\"# of GPU:\", torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Can run on GPU\") \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"can only run on CPU\")\n",
    "    \n",
    "print(\"Run on:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869478b0-a5fc-439b-be2b-fc7a7d9763f6",
   "metadata": {},
   "source": [
    "## Load the configration, and possibily adjust some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a2d884a-1c19-408b-81fa-9af42d2a604a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'images_root_dir': 'data/images',\n",
       "  'training_ids_file_path': 'data/train_ids.csv',\n",
       "  'validation_ids_file_path': 'data/val_ids.csv',\n",
       "  'test_ids_file_path': 'data/test_ids.csv',\n",
       "  'training_annotation_file_path': 'data/annotations/captions_train2014.json',\n",
       "  'test_annotation_file_path': 'data/annotations/captions_val2014.json',\n",
       "  'vocabulary_threshold': 10,\n",
       "  'img_size': (256, 256),\n",
       "  'batch_size': 20,\n",
       "  'num_workers': 4},\n",
       " 'model': {'hidden_size': 512,\n",
       "  'embedding_size': 300,\n",
       "  'model_type': 'baseline'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_data['dataset']['batch_size'] = 20\n",
    "config_data['dataset']['num_workers'] = 4\n",
    "\n",
    "config_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf6c5b58-d6ba-4a26-92f8-487ac844d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmentation = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=(256, 256),scale=(0.08, 1.0),ratio=(0.75, 1.3333)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=[-45.0, 45.0]),\n",
    "    transforms.RandomAffine(degrees=[-45.0, 45.0]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8c6028-6f29-4ddd-b407-7de23344dbd0",
   "metadata": {},
   "source": [
    "## Get dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f79c8d-e21a-4f36-8507-9db7394579c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.62s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created!\n",
      "Using the saved vocab.\n",
      "loading annotations into memory...\n",
      "Done (t=0.60s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.58s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.34s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# get dataloaders \n",
    "coco_test, vocabulary, train_data_loader, val_data_loader, test_data_loader = get_datasets(config_data, transform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63628d60-789b-4623-b102-7220196f8d42",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "#### lowest validation loss 1.46\n",
    "#### embed_size=512, hidden_size=600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8664a1-ca17-4063-86a8-225c54be493f",
   "metadata": {},
   "source": [
    "## Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "981f293a-898b-4223-9079-bdc14db79b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 1945\n",
      "embed_size: 500\n",
      "hidden_size: 1000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocabulary)\n",
    "embed_size = config_data['model']['embedding_size']\n",
    "hidden_size = config_data['model']['hidden_size']\n",
    "\n",
    "embed_size = 500 # 300\n",
    "hidden_size = 1000 # 512\n",
    "\n",
    "print(\"vocab_size:\", vocab_size) # see the vocab size for convininence to check dimensionalities\n",
    "print(\"embed_size:\", embed_size)\n",
    "print(\"hidden_size:\", hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7a2f94b-a39d-44d6-a52b-6460a921be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model\n",
    "encoder = baseline_encoder(embed_size).to(device)\n",
    "decoder = baseline_decoder(embed_size, hidden_size, vocab_size, device).to(device)\n",
    "\n",
    "# get training ready\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters())+list(encoder.resnet.fc.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.00005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# specify the name of checkpoints\n",
    "cp_name_encoder = \"LSTM_encoder.pt\"\n",
    "cp_name_decoder = \"LSTM_decoder.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f780203-bbac-4456-a564-853539b3e7e1",
   "metadata": {},
   "source": [
    "## Train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990d490-f381-43fa-bf97-a36cc4852466",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = [] # used to record training loss per epoch\n",
    "val_loss = [] # used to record validation loss per epoch\n",
    "best_val_loss = float('inf') # used to record the best validation loss for saving the best model\n",
    "\n",
    "for epoch in range(0, 20):\n",
    "    # turn the train mode on\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    # used to record losses per batch\n",
    "    # average them as the loss per epoch after the epoch\n",
    "    train_l_avg = [] \n",
    "    val_l_avg = []\n",
    "    \n",
    "    # go through the train set\n",
    "    for batch_idx, (image, target, img_ids) in enumerate(train_data_loader):\n",
    "        image, target = image.to(device), target.to(device) # set to GPU\n",
    "        \n",
    "        # zero gradients\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # get the image embedding\n",
    "        feature = encoder(image)\n",
    "        # pass the caption(target) and image embedding to the decoder\n",
    "        output = decoder(feature, target)\n",
    "        \n",
    "        # gradient descendent\n",
    "        loss = criterion(output.view(-1, vocab_size), target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_l_avg.append(loss.item()) # record loss per batch\n",
    "        \n",
    "    # average loss per batch to get loss per epoch\n",
    "    train_l = np.mean(train_l_avg) \n",
    "    train_loss.append(train_l)\n",
    "    \n",
    "    # turn the evaluation mode on\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        # go through the validation set\n",
    "        for batch_idx, (image, target, img_ids) in enumerate(val_data_loader):\n",
    "            image, target = image.to(device), target.to(device) # set to GPU\n",
    "            \n",
    "            feature = encoder(image) # get the image embedding\n",
    "            output = decoder(feature, target) # pass the caption(target) and image embedding to the decoder\n",
    "            \n",
    "            # get validation loss per batch\n",
    "            loss = criterion(output.view(-1, vocab_size), target.view(-1))\n",
    "            val_l_avg.append(loss.item())\n",
    "            \n",
    "    # average loss per batch to get loss per epoch\n",
    "    val_l = np.mean(val_l_avg)\n",
    "    val_loss.append(val_l)\n",
    "    \n",
    "    # if the validation loss is less, we have a better model\n",
    "    if val_l < best_val_loss:\n",
    "        best_val_loss = val_l\n",
    "        torch.save(encoder.state_dict(), cp_name_encoder)\n",
    "        torch.save(decoder.state_dict(), cp_name_decoder)\n",
    "        \n",
    "    #scheduler.step()\n",
    "            \n",
    "    print(\"Epoch\", epoch + 1, \"done:\")\n",
    "    print(\"training loss:\", train_l)\n",
    "    print(\"validation loss:\", val_l, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f0d65-5038-4285-b97c-a3cf7296f3b1",
   "metadata": {},
   "source": [
    "## Train and validation loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a4ce3-5af6-4c41-8c4c-2de365c769ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train/Validation Loss\")\n",
    "plt.legend([\"Training Loss\", \"Validation Loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d45a68-5869-40f7-ad74-b9a98f620db0",
   "metadata": {},
   "source": [
    "# Evaluate by Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7f903-4797-486e-96cc-f26f4299111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from caption_utils import bleu1, bleu4 # provided functions to calculate scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66b9c4-a156-402d-b153-cf93fe40d85b",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195bddb-9c31-4222-833c-0670b296b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = baseline_encoder(embed_size)\n",
    "decoder = baseline_decoder(embed_size, hidden_size, vocab_size, device)\n",
    "\n",
    "encoder.load_state_dict(torch.load(cp_name_encoder))\n",
    "decoder.load_state_dict(torch.load(cp_name_decoder))\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0558621-3180-4fc1-bbec-38273eace8f5",
   "metadata": {},
   "source": [
    "## Some functions helps us to evaluate BLEU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8f2fa-b1f7-464f-8d9d-167f5f40e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption2sentence(captions, vocabulary):\n",
    "    \"\"\"takes captions(list of word_idx) predicted by the model, and convert the captions to sentence(list of words)\"\"\"\n",
    "    # get a batch of captions (word idx)\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    for i in range(len(captions)): # iterate through batch\n",
    "        word_list = []\n",
    "        \n",
    "        for word_idx in captions[i]:\n",
    "            word_list.append(vocabulary.idx2word[word_idx]) \n",
    "            \n",
    "            if word_idx == 2: # if the word_idx == 2, <end>\n",
    "                break\n",
    "            \n",
    "        word_list = word_list[1:-1] # discard <start> and <end>\n",
    "        out.append(word_list)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f0b0c-8bee-42d0-b781-64e4517bf38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_caption(img_id, coco_test):\n",
    "    \"\"\"takes the img_id to get all captions(list of word_idx) of the image as reference to calcualte BLEU scores\"\"\"\n",
    "    out = []\n",
    "    for data in coco_test.imgToAnns[img_id]: # for each caption of given img_id\n",
    "        # tokenize the captions to make it compatible with the BLEU scores\n",
    "        caption_token = []\n",
    "        ref_cap = data[\"caption\"] \n",
    "        caption_tokenized = nltk.tokenize.word_tokenize(ref_cap.lower())\n",
    "        out.append(caption_tokenized)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e4bd32-0903-4289-b4ab-77a159ec9ecb",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "#### baseline: 1.49/67 BLEU1\n",
    "#### lowest loss: 1.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923c3bc-9a2f-450b-b5a7-47ff1d947cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the evaluation mode on\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "test_l_avg = [] # used to calculate test loss, stores loss per batch\n",
    "score1_avg = [] # used to calculate BLEU1, stores BLEU1 per batch\n",
    "score4_avg = [] # used to calculate BLEU4, stores BLEU4 per batch\n",
    "\n",
    "with torch.no_grad():\n",
    "    # go through the test set\n",
    "    for batch_idx, (image, target, img_ids) in enumerate(test_data_loader):\n",
    "        image, target = image.to(device), target.to(device) # set to GPU\n",
    "            \n",
    "        # forward the model\n",
    "        feature = encoder(image)\n",
    "        output = decoder(feature, target)\n",
    "            \n",
    "        # get loss per batch\n",
    "        loss = criterion(output.view(-1, vocab_size), target.view(-1))\n",
    "        test_l_avg.append(loss.item())\n",
    "        \n",
    "        # predict according to the image embedding\n",
    "        feat = feature.unsqueeze(1) # reshape to make it can be predicted by the model\n",
    "        predicted_captions = decoder.predict(feat)\n",
    "        # convert predictions(list of word_idx) into sentence(list of words)\n",
    "        predicted_captions = caption2sentence(predicted_captions, vocabulary)\n",
    "        \n",
    "        for i in range(len(predicted_captions)): # go through each element in the batch\n",
    "            predicted = predicted_captions[i]\n",
    "            im_id = img_ids[i] # get the img_id of the image\n",
    "            reference = get_reference_caption(im_id, coco_test) # get reference for the image\n",
    "            \n",
    "            # get scores\n",
    "            score1 = bleu1(reference, predicted)\n",
    "            score4 = bleu4(reference, predicted)\n",
    "            \n",
    "            # print some examples to make sure everything goes alright\n",
    "            if i == 0 and batch_idx== 0:\n",
    "                print(\"predicted\", predicted)\n",
    "                print(\"reference\", reference)\n",
    "                print(\"bleu1:\", score1)\n",
    "                print(\"bleu4:\", score4, \"\\n\")\n",
    "        \n",
    "            # record scores per element\n",
    "            score1_avg.append(score1)\n",
    "            score4_avg.append(score4)\n",
    "        \n",
    "print(\"Test loss is:\", np.mean(test_l_avg)) # test loss by averaging loss per batch\n",
    "# scores get by averaging scores of all images\n",
    "print(\"BLEU1 is:\", np.mean(score1_avg))\n",
    "print(\"BLEU4 is:\", np.mean(score4_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc6c32-6ec4-4f03-be8b-32c6a564ade5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27987573-7cdb-4e84-be90-a508ea1a8ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
